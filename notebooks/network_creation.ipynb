{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import os\n",
    "import networkx as nx\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts')\n",
    "\n",
    "from network import create_nodes, create_edges, save_nodes_as_csv, save_edges_as_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dataset can be downloaded from our cloud:\n",
    "> /GEODE/GEODE - Partage consortium/Corpus/EDdA/EDdA-geo-perdido-22.06.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v20221104\n"
     ]
    }
   ],
   "source": [
    "input_path = '/Users/lmoncla/Documents/Data/Corpus/EDDA/articles_geographie/perdido-22.06/'\n",
    "output_path = '../output/'\n",
    "outputSuffix = 'v' + date.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "print(outputSuffix)\n",
    "\n",
    "geocoding = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14452 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'd_headwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m         m \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+-(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\"\u001b[39m, file_id)\n\u001b[1;32m     13\u001b[0m         number \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mgroups()[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m         create_nodes(input_path, doc, number, geocoding)\n\u001b[1;32m     16\u001b[0m  \u001b[39m# create edges\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(input_path):\n",
      "File \u001b[0;32m~/git/github.com/GEODE/edda-network/notebooks/../scripts/network.py:32\u001b[0m, in \u001b[0;36mcreate_nodes\u001b[0;34m(file_path, filename, number, geocoding)\u001b[0m\n\u001b[1;32m     28\u001b[0m     author \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[39m''' on normalise et stock le noeud dans un dictionnaire '''\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m k \u001b[39m=\u001b[39m norm_headwords(head_value\u001b[39m.\u001b[39;49mlower())\n\u001b[1;32m     34\u001b[0m alt_names \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(d_headwords[k])\n\u001b[1;32m     36\u001b[0m \u001b[39m#nbwords = sum(1 for _ in div1.findall('.//w'))\u001b[39;00m\n",
      "File \u001b[0;32m~/git/github.com/GEODE/edda-network/notebooks/../scripts/network.py:268\u001b[0m, in \u001b[0;36mnorm_headwords\u001b[0;34m(head_value)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mfor\u001b[39;00m name, altNames \u001b[39min\u001b[39;00m headword\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    267\u001b[0m     k \u001b[39m=\u001b[39m name\n\u001b[0;32m--> 268\u001b[0m     d_headwords[name] \u001b[39m=\u001b[39m []\n\u001b[1;32m    269\u001b[0m     d_headwords[name]\u001b[39m.\u001b[39mappend(name)\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m altNames:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd_headwords' is not defined"
     ]
    }
   ],
   "source": [
    "d_headwords = {}\n",
    "headwords = []\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# create nodes\n",
    "for doc in tqdm(os.listdir(input_path)):\n",
    "    file_id = doc[:-4]\n",
    "    extension = doc[-4:]\n",
    "\n",
    "    if extension == '.xml':\n",
    "        m = re.match(\"\\w+-(\\d+)\", file_id)\n",
    "        number = m.groups()[0]\n",
    "        create_nodes(input_path, doc, number, d_headwords, geocoding)\n",
    "\n",
    " # create edges\n",
    "for doc in os.listdir(input_path):\n",
    "    file_id = doc[:-4]\n",
    "    # print('artcile ' + file_id)\n",
    "    extension = doc[-4:]\n",
    "\n",
    "    if extension == '.xml':\n",
    "        m = re.match(\"\\w+-(\\d+)\", file_id)\n",
    "        number = m.groups()[0]\n",
    "        create_edges(input_path, doc, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save graph\n",
    "nx.write_gexf(G, output_path + 'network-'+outputSuffix+'.gexf')\n",
    "\n",
    "# save_edges_as_csv(output_path + 'edges-'+outputSuffix+'.csv', ';', G.edges)\n",
    "save_nodes_as_csv(output_path + 'nodes-' + outputSuffix + '.tsv', '\\t', G.nodes)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Geoparsing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "736d210290d6fb1193e83af4d102c72889918b431f5fb98003776661da6f3cb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
